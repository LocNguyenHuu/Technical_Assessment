# Assignment: Building a Scalable Web Crawler System

## Introduction:
As a senior backend developer, you have been assigned the task of designing and implementing a scalable web crawler system. 
You can crawl any website you like, but it should be complicated. The system should be able to efficiently crawl and extract information from web pages. This assignment aims to test your proficiency and skills in backend development, particularly in designing and implementing scalable systems, handling different types of data, managing distributed systems, and implementing algorithms for information extraction and processing.

## Task 1: Database Design and Management


Design a database schema to store the crawled data efficiently.

Implement a database management system (e.g., MySQL, PostgreSQL) to store and query the crawled data.

Ensure the database schema is optimized for performance and can handle a large volume of data.

## Task 2: Web Crawling Algorithm


Develop a web crawling algorithm that can efficiently crawl web pages and extract relevant information.

Implement a distributed crawling system that can handle crawling multiple websites simultaneously.

Ensure the crawling algorithm is efficient, reliable, and scalable.

## Task 3: Data Extraction and Processing


Develop algorithms to extract and process information from crawled web pages.

Implement a mechanism to handle different types of data (e.g., text, images, videos).

Ensure the extracted data is accurate, consistent, and properly formatted.

## Task 4: Distributed System Management


Implement a mechanism to manage and coordinate multiple crawling instances.

Develop strategies to handle distributed crawling, such as load balancing and fault tolerance.

Ensure the system can handle failures and recover gracefully.

### Notes
In task 4: Please write a simple high level architecture in your README.md

## Task 5: API Development and Documentation


Design and implement a RESTful API to expose the crawled data.

Document the API endpoints, request/response formats, and authentication mechanisms.

Ensure the API is secure, well-documented, and follows best practices.

## Task 6: Docker Deployment


Dockerize the web crawler system for easy deployment and scalability.

Create Docker containers for all the components of the system (e.g., database, crawling engine).

Use Docker Compose for container orchestration and easy setup.

## Task 7: Unit Testing and Test Coverage


Write unit tests to ensure the correctness of the implemented functionalities.

Achieve a high test coverage to ensure the reliability and robustness of the system.

Use a testing framework (e.g., pytest) and follow best practices for unit testing.

## Task 8: Scalability and Performance Optimization


Identify potential bottlenecks in the system and propose optimizations.

Implement strategies to improve the scalability and performance of the system.

Conduct load testing to evaluate the system's performance under high load.

## Task 9: Documentation


Create comprehensive documentation that explains the architecture, design decisions, and implementation details of the web crawler system.

Include clear instructions on how to set up and run the system locally.

Provide guidelines on how to deploy the system in a production environment.

## Evaluation Criteria:


Correctness: The system should accurately crawl and extract information from web pages.

Scalability: The system should be able to handle crawling multiple websites concurrently and manage distributed crawling instances.

Efficiency: The crawling algorithm and data processing algorithms should be efficient and performant.

Database Design: The database schema and management system should be optimized for performance and scalability.

API Development: The API endpoints should adhere to RESTful principles and be well-documented.

Docker Deployment: The system should be properly Dockerized and easy to deploy using Docker Compose.

Unit Testing: The system should have comprehensive unit tests with high code coverage.

Scalability and Performance: The system should be scalable and perform well under high load.

Documentation: The documentation should be clear, comprehensive, and provide sufficient instructions for setup and deployment.

## Submission Guidelines:


Create a GitHub repository to host your code and documentation.

Provide a demo video record on Google drive.

Include a README file with instructions on how to set up and run the system.

Include a detailed documentation file that explains the system's architecture, design decisions, and implementation details.

Commit your code regularly with meaningful commit messages.

Submit the GitHub repository link along with any additional instructions or notes.

## Additional Notes:
To make the assignment more challenging, you can consider adding features like handling JavaScript-rendered pages, implementing a distributed task queue, employing machine learning algorithms for information extraction, or integrating with external services (e.g., Elasticsearch for indexing).

Good luck with the assignment!